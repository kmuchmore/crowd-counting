{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crowd Counting\n",
    "by David Ohm\n",
    "\n",
    "This notebook will step you through the process of training and testing a deep learning model for estimating crowd density. The output heatmap (i.e., density map) can be used to compute an estimate of the total number of peaople in an image. Note - the same model could be trained to count other objects (i.e., vehicles, fruit, microscopic organisms, etc.). \n",
    "\n",
    "The model in this notebook is based on the model in the paper [CSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes](https://arxiv.org/abs/1802.10062).\n",
    "\n",
    "### Results:\n",
    "\n",
    "TBD   \n",
    "\n",
    "### Dataset:\n",
    "\n",
    "ShanghaiTech dataset\n",
    "\n",
    "### Training Parameters:\n",
    "\n",
    "1. *Loss* = MSE;\n",
    "\n",
    "2. *Optimizer* = SGD(lr=1e-6);\n",
    "\n",
    "3. *Batch size*: 1;\n",
    "\n",
    "4. *Data augmentation*: Flip horizontally randomly;\n",
    "\n",
    "5. *Weights*: Got best weights of SHB in epoch xxx, the best one of SHA in epoch xxx   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm as CM\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import math\n",
    "import glob\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, multiply, BatchNormalization, ReLU, Activation\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import tensorflow as tf\n",
    "\n",
    "if tf.__version__ != '2.0.0':\n",
    "    tf.compat.v1.enable_v2_behavior()\n",
    "    tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scroll down to see TF2 Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "root = pathlib.Path('/data/crowd_counting/ShanghaiTech/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_A_train = root.joinpath('part_A/train_data','images')\n",
    "part_A_test = root.joinpath('part_A/test_data','images')\n",
    "part_B_train = root.joinpath('part_B/train_data','images')\n",
    "part_B_test = root.joinpath('part_B/test_data','images')\n",
    "\n",
    "temp = 'test_images'\n",
    "train_path_sets = [part_B_train]\n",
    "test_path_sets = [part_B_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [str(path) for path in part_B_train.glob('*.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,7)   # set plot sizes\n",
    "print('File[100] path is: ', files[100])\n",
    "img = plt.imread(files[100])\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Image shape is: ', img.shape)\n",
    "print('Image type is: ', img.dtype)\n",
    "print('min/max size = ', np.min(img), np.max(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training image paths\n",
    "train_img_paths = []\n",
    "\n",
    "for path in train_path_sets:\n",
    "    img_paths = [str(img_path) for img_path in path.glob('*.jpg')]\n",
    "    train_img_paths.extend(img_paths)\n",
    "        \n",
    "print(\"Total images : \",len(train_img_paths))\n",
    "print(\"Example path is: \", train_img_paths[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test image paths\n",
    "test_img_paths = []\n",
    "\n",
    "for path in test_path_sets:\n",
    "    img_paths = [str(img_path) for img_path in path.glob('*.jpg')]\n",
    "    test_img_paths.extend(img_paths)\n",
    "        \n",
    "print(\"Total images : \",len(test_img_paths))\n",
    "print(\"Example path is: \", test_img_paths[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup some variables\n",
    "num_train_images = len(train_img_paths)\n",
    "num_test_images = len(test_img_paths)\n",
    "print('Number of training images: ', num_train_images)\n",
    "print('Number of test images: ', num_test_images)\n",
    "batch_size = 1\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_input(path):\n",
    "    #Function to load,normalize and return image \n",
    "    im = tf.io.read_file(path)\n",
    "    im = tf.image.decode_jpeg(im, channels=3)\n",
    "#     Image.open(path).convert('RGB') \n",
    "#     im = np.array(im)\n",
    "    #print('min/max size = ', np.min(jnk), np.max(jnk))   \n",
    "    im = tf.cast(im, tf.float32) / 255.0\n",
    "    \n",
    "    channels = tf.unstack(im, axis=2)\n",
    "        \n",
    "    channels[0]=(channels[0]-0.485)/0.229\n",
    "    channels[1]=(channels[1]-0.456)/0.224\n",
    "    channels[2]=(channels[2]-0.406)/0.225\n",
    "    \n",
    "    im = tf.stack(channels, axis=2)\n",
    "    \n",
    "    #im = im.astype('float32')\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnk = get_input(train_img_paths[100])\n",
    "print('Image shape is: ', jnk.shape)\n",
    "print('Image type is: ', jnk.dtype)\n",
    "print('min/max size = ', np.min(jnk[:,:,1]), np.max(jnk[:,:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(path):\n",
    "    #import target density map and resize target to 1/8 input size since netwrok downsamples to 1/8 input size\n",
    "    gt_file = h5py.File(path,'r')\n",
    "    \n",
    "    target = tf.constant(gt_file['density'], tf.float32)\n",
    "    ## Note - multiply by 64 due to 1/8 downsample, then multiply by another 64 to boost pixel values in order to train (ignore edge artifacts)\n",
    "    img = tf.expand_dims(target,axis=2)\n",
    "    img = tf.image.resize(img,(int(target.shape[0]/8),int(target.shape[1]/8)),method=tf.image.ResizeMethod.BICUBIC)*64*64 \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_input_hm(path):\n",
    "#     #import target density map and resize target to 1/8 input size since netwrok downsamples to 1/8 input size\n",
    "#     gt_file = h5py.File(path,'r')\n",
    "#     target = np.asarray(gt_file['density'])\n",
    "#     img = cv2.resize(target,(int(target.shape[1]),int(target.shape[0])),interpolation = cv2.INTER_LINEAR)*1024*64\n",
    "     \n",
    "#     img = np.expand_dims(img,axis  = 2) \n",
    "#     #img = np.stack((img,)*3, axis=-1)    \n",
    "#     #print(img.shape)\n",
    "    \n",
    "#     return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnk = get_output('/data/crowd_counting/ShanghaiTech/part_B/train_data/ground/IMG_148.h5')\n",
    "jnk.shape\n",
    "#jnk = get_input_hm('/data/crowd_counting/ShanghaiTech/part_B/train_data/ground/IMG_148.h5')\n",
    "#jnk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test this last function's calls and look at a sample density plot (i.e. heatmap) and get ground-truth count\n",
    "test_heatmap_path = train_img_paths[100].replace('.jpg','.h5').replace('images','ground')\n",
    "test_heatmap_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_file = h5py.File(test_heatmap_path,'r')\n",
    "# target = np.asarray(gt_file['density'])\n",
    "# img = cv2.resize(target,(int(target.shape[1]/8),int(target.shape[0]/8)),interpolation = cv2.INTER_CUBIC)*64   ## 8 x 8 = 64 \n",
    "# plt.imshow(img,cmap=CM.jet)\n",
    "# print(img.shape)\n",
    "# print(\"Ground-Truth Sum = \" ,np.sum(img))\n",
    "img = tf.squeeze(get_output(test_heatmap_path)).numpy() / 64\n",
    "print(\"Ground-Truth Sum = \" ,np.sum(img))\n",
    "plt.imshow(img,cmap=CM.jet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Image shape is: ', img.shape)\n",
    "print('Image type is: ', img.dtype)\n",
    "print('min/max size = ', np.min(img), np.max(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def preprocess_input(image,target):\n",
    "#    #crop image and crop target\n",
    "#    #resize target\n",
    "#    crop_size = (int(image.shape[1]/2),int(image.shape[2]/2))\n",
    "#    crop_size_target = (int(target.shape[1]/2),int(target.shape[2]/2))\n",
    "    \n",
    "    \n",
    "#    if random.randint(0,9)<= 1:            \n",
    "#            dx = int(random.randint(0,1)*image.shape[1]*1./2)\n",
    "#            dy = int(random.randint(0,1)*image.shape[2]*1./2)\n",
    "#            dxt = int(random.randint(0,1)*target.shape[1]*1./2)\n",
    "#            dyt = int(random.randint(0,1)*target.shape[2]*1./2)\n",
    "#            \n",
    "#    else:\n",
    "#            dx = int(random.random()*image.shape[1]*1./2)\n",
    "#            dy = int(random.random()*image.shape[2]*1./2)\n",
    "#            dxt = int(random.random()*target.shape[1]*1./2)\n",
    "#            dyt = int(random.random()*target.shape[2]*1./2)\n",
    "#\n",
    "#    #print(crop_size , dx , dy)\n",
    "#    img = image[:, dx:crop_size[0]+dx , dy:crop_size[1]+dy, :]\n",
    "#    target_aug = target[:,dxt:crop_size_target[0]+dxt,dyt:crop_size_target[1]+dyt,:]\n",
    "#    \n",
    "#    print(img.shape)\n",
    "#    print(target_aug.shape)\n",
    "#\n",
    "#    return(img,target_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def flip_horizontally(x, y):\n",
    "#     to_flip = np.random.randint(0, 2)\n",
    "#     if to_flip:\n",
    "#         #x = np.squeeze(x)\n",
    "#         #y = np.squeeze(y)\n",
    "#         #x, y = cv2.flip(x, 1), np.expand_dims(cv2.flip(np.squeeze(y), 1), axis=-1)\n",
    "#         x = cv2.flip(x, 1)\n",
    "#         y = cv2.flip(y, 1)\n",
    "#         # Suppose shape of y is (123, 456, 1), after cv2.flip, shape of y would turn into (123, 456).\n",
    "#     return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF2 Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a TFRecord\n",
    "NOTE: THIS TAKES A WHILE (~5 minutes), SKIP THIS IF THE RECORD HAS ALREADY BEEN CREATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training images:  400\n",
      "Example train path is:  /data/crowd_counting/ShanghaiTech/part_B/train_data/images/IMG_148.jpg\n",
      "Total training images:  316\n",
      "Example test path is:   /data/crowd_counting/ShanghaiTech/part_B/test_data/images/IMG_278.jpg\n"
     ]
    }
   ],
   "source": [
    "root = pathlib.Path('/data/crowd_counting/ShanghaiTech/')\n",
    "part_A_train = root.joinpath('part_A/train_data','images')\n",
    "part_A_test = root.joinpath('part_A/test_data','images')\n",
    "part_B_train = root.joinpath('part_B/train_data','images')\n",
    "part_B_test = root.joinpath('part_B/test_data','images')\n",
    "\n",
    "def get_images_in_path_sets(path_sets):\n",
    "    all_img_paths = []\n",
    "    for path_set in path_sets:\n",
    "        img_paths = [str(img_path) for img_path in path_set.glob('*.jpg')]\n",
    "        all_img_paths.extend(img_paths)\n",
    "    return all_img_paths\n",
    "\n",
    "train_img_paths = get_images_in_path_sets([part_B_train])\n",
    "print(\"Total training images: \",len(train_img_paths))\n",
    "print(\"Example train path is: \", train_img_paths[100])\n",
    "\n",
    "test_img_paths = get_images_in_path_sets([part_B_test])\n",
    "print(\"Total training images: \",len(test_img_paths))\n",
    "print(\"Example test path is:  \", test_img_paths[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    \"\"\"Wrapper for insert int64 feature into Example proto.\"\"\"\n",
    "    if not isinstance(value, list) and not isinstance(value, np.ndarray):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Wrapper for insert float features into Example proto.\"\"\"\n",
    "    if not isinstance(value, list) and not isinstance(value, np.ndarray):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Wrapper for insert bytes features into Example proto.\"\"\"\n",
    "    if not isinstance(value, list) and not isinstance(value, np.ndarray):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(path):\n",
    "    # Read in image, encode it as a png\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "\n",
    "    # Read in label and convert it to numpy array\n",
    "    gt_file = h5py.File(path.replace('.jpg','.h5').replace('images','ground'),'r')\n",
    "    label = tf.constant(gt_file['density'], tf.float32)\n",
    "    \n",
    "    #TFRecords hold 1D arrays, so store a flattened version of the label along with the original shape\n",
    "    example = tf.train.Example(\n",
    "        features=tf.train.Features(\n",
    "            feature={\n",
    "                'image': _bytes_feature(tf.image.encode_png(img, compression=0).numpy()),\n",
    "                'label': _bytes_feature(tf.io.serialize_tensor(label).numpy()),\n",
    "            }                          \n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Training Record at train.record\n"
     ]
    }
   ],
   "source": [
    "train_record_path = \"train.record\"\n",
    "print(\"Creating Training Record at {}\".format(str(train_record_path)))\n",
    "\n",
    "# Create the Train TFRecord\n",
    "writer = tf.io.TFRecordWriter(train_record_path)\n",
    "for data in train_img_paths:\n",
    "    example = serialize_example(data)\n",
    "    writer.write(example.SerializeToString())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Test Record at test.record\n"
     ]
    }
   ],
   "source": [
    "test_record_path = \"test.record\"\n",
    "print(\"Creating Test Record at {}\".format(str(test_record_path)))\n",
    "\n",
    "# Create the Test TFRecord\n",
    "writer = tf.io.TFRecordWriter(test_record_path)\n",
    "for data in test_img_paths:\n",
    "    example = serialize_example(data)\n",
    "    writer.write(example.SerializeToString())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define how to preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'function'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9909c8cbe0b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     features = {\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFixedLenFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# one image one record\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m'label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFixedLenFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'function'"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def parse_record(serialized_example):\n",
    "    features = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),  # one image one record\n",
    "        'label': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(serialized_example, features)\n",
    "\n",
    "    img = tf.io.decode_png(example['image'])\n",
    "    label = tf.io.parse_tensor(example['label'], out_type=tf.float32)\n",
    "\n",
    "    img.set_shape([768, 1024, 3])\n",
    "    label.set_shape([768, 1024])\n",
    "    \n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def preprocess(img, label):\n",
    "    \n",
    "    # Cast the image to float32\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "\n",
    "    # Split the channels appart to apply operations to each channel seperately\n",
    "    channels = tf.unstack(img, axis=2)\n",
    "    \n",
    "    channels[0]=(channels[0]-0.485)/0.229\n",
    "    channels[1]=(channels[1]-0.456)/0.224\n",
    "    channels[2]=(channels[2]-0.406)/0.225\n",
    "    \n",
    "    # Stack the channels together again to create the image\n",
    "    img = tf.stack(channels, axis=2)\n",
    "        \n",
    "    # tf.image.resize requires a 3 or 4 dimensionals (h,w,c) or (b,h,w,c)\n",
    "    label = tf.expand_dims(label,axis=2)\n",
    "\n",
    "    # resize label density map to 1/8 input size since network downsamples to 1/8 input size\n",
    "    ## Note - multiply by 64 due to 1/8 downsample, then multiply by another 64 to boost pixel values in order to train (ignore edge artifacts)\n",
    "    label = tf.image.resize(label,(int(label.shape[0]/8),int(label.shape[1]/8)),method=tf.image.ResizeMethod.BICUBIC)*64*64\n",
    "\n",
    "    do_flip = tf.random.uniform([]) > 0.5\n",
    "    if do_flip:\n",
    "        tf.image.flip_left_right(img)\n",
    "        tf.image.flip_left_right(label)\n",
    "\n",
    "    print(\"Image shape: \", img.shape)\n",
    "    print(\"Label shape: \", label.shape)\n",
    "    \n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and testing datasets and map preprocessing functions to them  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_record_path = \"train.record\"\n",
    "train_ds = tf.data.TFRecordDataset(train_record_path)\n",
    "train_ds = train_ds.map(parse_record, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_ds = train_ds.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# train_ds = train_ds.shuffle(32).batch(1).prefetch(1)\n",
    "train_ds = train_ds.shuffle(32).repeat().batch(4).prefetch(1)\n",
    "\n",
    "# The repeat() is needed for TF 1.14 because of a bug where the dataset doesn't loop automatically when using the distribute strategy\n",
    "# When not using distribute strategy (Training on a single GPU) this is not required and is fixed in TF 2.0\n",
    "\n",
    "test_record_path = \"test.record\"\n",
    "test_ds = tf.data.TFRecordDataset(test_record_path)\n",
    "test_ds = test_ds.map(parse_record, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_ds = test_ds.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# test_ds = test_ds.shuffle(32).batch(1).prefetch(1)\n",
    "test_ds = test_ds.shuffle(32).repeat().batch(4).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View some of the training data\n",
    "for (img_batch, label_batch) in train_ds.take(1):\n",
    "    img, label = img_batch[0], tf.squeeze(label_batch[0]) # Index 0 because we are now batched\n",
    "    \n",
    "    f, axarr = plt.subplots(1,2)\n",
    "    axarr[0].imshow(img.numpy())\n",
    "    axarr[1].imshow(label.numpy(),cmap=CM.jet)\n",
    "    plt.show()\n",
    "    print(\"Ground-Truth Sum = \" ,np.sum(label.numpy() / 64))\n",
    "    print('Image shape is: ', img.numpy().shape)\n",
    "    print('Image type is: ', img.numpy().dtype)\n",
    "    print('min/max size = ', np.min(img.numpy()), np.max(img.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def euclidean_distance_loss(y_true, y_pred):\n",
    "    # Euclidean distance as a measure of loss (Loss function) \n",
    "    sub_var = tf.math.square(y_pred - y_true)\n",
    "    #tf.print('Sub_var: ', sub_var)\n",
    "    sum_square = tf.math.reduce_sum(sub_var)\n",
    "    #tf.print('Sum_square: ', sum_square.shape, sum_square)\n",
    "    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network model : VGG + Conv\n",
    "def CrowdNet():  \n",
    "            #Variable Input Size\n",
    "            rows = None\n",
    "            cols = None\n",
    "            channels = 3\n",
    "            \n",
    "            #Batch Normalisation option\n",
    "            \n",
    "            #batch_norm = 0\n",
    "            kernel = (3, 3)\n",
    "            init = RandomNormal(stddev=0.01)\n",
    "            \n",
    "            \n",
    "            \n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(64, kernel_size = kernel,activation = 'relu', padding='same',input_shape = (rows, cols, channels), kernel_initializer = init))\n",
    "            model.add(Conv2D(64, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
    "            model.add(MaxPooling2D(strides=2))\n",
    "            model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
    "            model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
    "            model.add(MaxPooling2D(strides=2))\n",
    "            model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
    "            model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
    "            model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
    "            model.add(MaxPooling2D(strides=2))            \n",
    "            model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
    "            model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
    "            model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                \n",
    "                \n",
    "            #Conv2D\n",
    "            model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(256, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(128, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(64, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(1, (1, 1), activation='relu', dilation_rate = 1, kernel_initializer = init, padding = 'same', name = 'output'))\n",
    "        \n",
    "            front_end = VGG16(weights='imagenet', include_top=False)\n",
    "            \n",
    "            weights_front_end = []\n",
    "            for layer in front_end.layers:\n",
    "                if 'conv' in layer.name:\n",
    "                    weights_front_end.append(layer.get_weights())\n",
    "            \n",
    "            counter_conv = 0\n",
    "            for i in range(len(model.layers)):\n",
    "                if counter_conv >= 13:\n",
    "                    break\n",
    "                if 'conv' in model.layers[i].name:\n",
    "                    model.layers[i].set_weights(weights_front_end[counter_conv])\n",
    "                    counter_conv += 1\n",
    "            \n",
    "            #model = init_weights_vgg(model)\n",
    "        \n",
    "            return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    model = CrowdNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    sgd = SGD(lr = 1e-6, decay = (1e-4), momentum = 0.95)\n",
    "    #adam = Adam(lr = 1e-4, decay = (5*1e-3))\n",
    "    model.compile(optimizer=sgd, loss=euclidean_distance_loss, metrics=['mse'])\n",
    "    #model.compile(optimizer=adam, loss=euclidean_distance_loss, metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random((1,768,1024,3)).astype('float32')\n",
    "#x = np.random.random((1,768,1024,1)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model according to the conditions \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "# model_save_path = '/data/crowd_counting/saved_models/'\n",
    "model_name = 'crowd_counting_model-dist'\n",
    "cp_str = \"training/checkpoints/\"+model_name+\"/epoch{epoch:02d}-loss{loss:.2f}-val_loss{val_loss:.2f}.h5\"\n",
    "# checkpoint = ModelCheckpoint(model_save_path+model_name+\"_epoch{epoch:02d}-loss{loss:.2f}.h5\", monitor='loss', verbose=1, save_best_only=True,  mode='max' )\n",
    "os.makedirs(\"training/checkpoints/{}\".format(model_name), exist_ok=True)\n",
    "callbacks = [\n",
    "    ModelCheckpoint(cp_str),\n",
    "    TensorBoard(\"training/logs/{}\".format(model_name))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "print(\"Training...\")\n",
    "H = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=test_ds,\n",
    "    steps_per_epoch = len(train_img_paths) // 4,\n",
    "    validation_steps = 75,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "N = num_epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "#plt.plot(np.arange(0, N), H.history[\"mse\"], label=\"train_mse\")\n",
    "#plt.plot(np.arange(0, N), H.history[\"val_mean_squared_error\"], label=\"val_mse\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_mod(model,\"/data/crowd_counting/weights/model_A_weights.h5\",\"/data/crowd_counting/models/Model_A.json\")\n",
    "#model.save('/data/crowd_counting/saved_models/crowd_counting_modelA_2_51epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_model():\n",
    "#     # Function to load and return neural network model \n",
    "#     json_file = open('/data/crowd_counting/models/Model_B.json', 'r')\n",
    "#     #json_file = open('/data/crowd_counting/models/ModelA.json', 'r')\n",
    "#     loaded_model_json = json_file.read()\n",
    "#     json_file.close()\n",
    "#     loaded_model = model_from_json(loaded_model_json)\n",
    "#     loaded_model.load_weights(\"/data/crowd_counting/weights/model_B_weights.h5\")\n",
    "#     #loaded_model.load_weights(\"/dml/notebooks/CSRNet-keras/weights/pretrained.h5\")\n",
    "#     return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_img_test(path):\n",
    "    #Function to load,normalize and return image \n",
    "    im = Image.open(path)\n",
    "    im = np.array(im)\n",
    "    \n",
    "    im = im/255.0\n",
    "    \n",
    "    im[:,:,0]=(im[:,:,0]-0.485)/0.229\n",
    "    im[:,:,1]=(im[:,:,1]-0.456)/0.224\n",
    "    im[:,:,2]=(im[:,:,2]-0.406)/0.225\n",
    "\n",
    "    im = np.expand_dims(im, axis = 0)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try some predictions to view output of model\n",
    "# test_m = tf.keras.models.load_model(\"training/checkpoints/crowd_counting_model-dist/epoch60-loss217.69.h5\", compile=False)\n",
    "test_m = tf.keras.models.load_model(\"training/checkpoints/crowd_counting_model-dist/epoch90-loss204.93.h5\", compile=False)\n",
    "def predict(path):\n",
    "    #Function to load image,predict heat map, generate count and return (count , image , heat map)\n",
    "    #model = load_model()\n",
    "    image = create_img_test(path)\n",
    "    ans = test_m.predict(image)\n",
    "    count = np.sum(ans)\n",
    "    return count,image,ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use a test image to check the prediction (e.g., inference)\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)   # set plot sizes\n",
    "test_image = test_img_paths[31]\n",
    "# test_image = '/data/crowd_counting/ShanghaiTech/part_B/test_data/images/IMG_145.jpg'\n",
    "test_heatmap_truth = test_image.replace('.jpg','.h5').replace('images','ground')\n",
    "## Get the Ground Truth heatmap\n",
    "gt_file = h5py.File(test_heatmap_truth,'r')\n",
    "groundtruth = np.asarray(gt_file['density'])\n",
    "groundtruth = cv2.resize(groundtruth,(int(groundtruth.shape[1]/8),int(groundtruth.shape[0]/8)),interpolation = cv2.INTER_CUBIC)*64   ## 8 x 8 = 64\n",
    "#plt.imshow(groundtruth,cmap=CM.jet)\n",
    "print(\"Ground Truth People Count = \" ,int(np.sum(groundtruth)))\n",
    "#print('Ground Truth Heat Map Dimensions: ', groundtruth.shape)\n",
    "#print('Image type is: ', groundtruth.dtype)\n",
    "#print('min/max size = ', np.min(groundtruth), np.max(groundtruth))\n",
    "#plt.show()\n",
    "\n",
    "# get the input image\n",
    "count,image,hmap = predict(test_image)\n",
    "width = image.shape[2]\n",
    "height = image.shape[1]\n",
    "channels = image.shape[3]\n",
    "print('Input Image Path: ', test_image)\n",
    "#print('Input Image dimensions: ', width, 'x ', height, 'x ', channels)\n",
    "input_image = Image.open(test_image)\n",
    "plt.imshow(input_image)\n",
    "#plt.imshow(image.reshape(height,width,channels))\n",
    "plt.show()\n",
    "\n",
    "# predicted heat map info \n",
    "hwidth = hmap.shape[2]\n",
    "hheight = hmap.shape[1]\n",
    "hchannels = hmap.shape[3]\n",
    "#print('Output Heat Map Dimensions: ', hwidth, 'x ', hheight, 'x ', hchannels)\n",
    "print('Estimated People Count = ', int(count) / 64)\n",
    "plt.imshow(hmap.reshape(hheight,hwidth), cmap = CM.jet )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
