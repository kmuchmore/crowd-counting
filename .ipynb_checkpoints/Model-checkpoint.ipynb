{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.python.keras.preprocessing.image import load_img,img_to_array\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.python.keras.initializers import RandomNormal\n",
    "from tensorflow.python.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.python.keras.optimizers import SGD\n",
    "from tensorflow.python.keras.models import Model,Sequential\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import model_from_json\n",
    "from matplotlib import cm as CM\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import scipy.io as io\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import h5py\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import random\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "root = '/data/crowd_counting/ShanghaiTech/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_A_train = os.path.join(root,'part_A/train_data','images')\n",
    "part_A_test = os.path.join(root,'part_A/test_data','images')\n",
    "part_B_train = os.path.join(root,'part_B/train_data','images')\n",
    "part_B_test = os.path.join(root,'part_B/test_data','images')\n",
    "\n",
    "temp = '/data/crowd_counting/test_images'\n",
    "path_sets = [part_A_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images :  300\n"
     ]
    }
   ],
   "source": [
    "img_paths = []\n",
    "\n",
    "for path in path_sets:\n",
    "    \n",
    "    for img_path in glob.glob(os.path.join(path, '*.jpg')):\n",
    "        \n",
    "        img_paths.append(str(img_path))\n",
    "        \n",
    "print(\"Total images : \",len(img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_img(path):\n",
    "    #Function to load,normalize and return image \n",
    "    im = Image.open(path).convert('RGB')\n",
    "    \n",
    "    im = np.array(im)\n",
    "    \n",
    "    im = im/255.0\n",
    "    \n",
    "    im[:,:,0]=(im[:,:,0]-0.485)/0.229\n",
    "    im[:,:,1]=(im[:,:,1]-0.456)/0.224\n",
    "    im[:,:,2]=(im[:,:,2]-0.406)/0.225\n",
    "\n",
    "    #print(im.shape)\n",
    "    #im = np.expand_dims(im,axis  = 0)\n",
    "    return im\n",
    "\n",
    "def get_input(path):\n",
    "    path = path[0] \n",
    "    img = create_img(path)\n",
    "    return(img)\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_output(path):\n",
    "    #import target\n",
    "    #resize target\n",
    "    \n",
    "    gt_file = h5py.File(path,'r')\n",
    "    \n",
    "    target = np.asarray(gt_file['density'])\n",
    "    \n",
    "    img = cv2.resize(target,(int(target.shape[1]/8),int(target.shape[0]/8)),interpolation = cv2.INTER_CUBIC)*64\n",
    "    \n",
    "    img = np.expand_dims(img,axis  = 3)\n",
    "    \n",
    "    #print(img.shape)\n",
    "    \n",
    "    return img\n",
    "    \n",
    "    \n",
    "    \n",
    "def preprocess_input(image,target):\n",
    "    #crop image\n",
    "    #crop target\n",
    "    #resize target\n",
    "    crop_size = (int(image.shape[0]/2),int(image.shape[1]/2))\n",
    "    \n",
    "    \n",
    "    if random.randint(0,9)<= -1:            \n",
    "            dx = int(random.randint(0,1)*image.shape[0]*1./2)\n",
    "            dy = int(random.randint(0,1)*image.shape[1]*1./2)\n",
    "    else:\n",
    "            dx = int(random.random()*image.shape[0]*1./2)\n",
    "            dy = int(random.random()*image.shape[1]*1./2)\n",
    "\n",
    "    #print(crop_size , dx , dy)\n",
    "    img = image[dx : crop_size[0]+dx , dy:crop_size[1]+dy]\n",
    "    \n",
    "    target_aug = target[dx:crop_size[0]+dx,dy:crop_size[1]+dy]\n",
    "    #print(img.shape)\n",
    "\n",
    "    return(img,target_aug)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image data generator \n",
    "def image_generator(files, batch_size = 64):\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        input_path = np.random.choice(a = files, size = batch_size)\n",
    "        \n",
    "        batch_input = []\n",
    "        batch_output = [] \n",
    "          \n",
    "        #for input_path in batch_paths:\n",
    "        \n",
    "        inputt = get_input(input_path )\n",
    "        output = get_output(input_path[0].replace('.jpg','.h5').replace('images','ground') )\n",
    "            \n",
    "       \n",
    "        batch_input += [inputt]\n",
    "        batch_output += [output]\n",
    "    \n",
    "\n",
    "        batch_x = np.array( batch_input )\n",
    "        batch_y = np.array( batch_output )\n",
    "        \n",
    "        yield( batch_x, batch_y )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mod(model , str1 , str2):\n",
    "    model.save_weights(str1)\n",
    "    \n",
    "    model_json = model.to_json()\n",
    "    \n",
    "    with open(str2, \"w\") as json_file:\n",
    "        json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_vgg(model):\n",
    "    #vgg =  VGG16(weights='imagenet', include_top=False)\n",
    "    \n",
    "    json_file = open('models/VGG_16.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(\"weights/VGG_16.h5\")\n",
    "    \n",
    "    vgg = loaded_model\n",
    "    \n",
    "    vgg_weights=[]                         \n",
    "    for layer in vgg.layers:\n",
    "        if('conv' in layer.name):\n",
    "            vgg_weights.append(layer.get_weights())\n",
    "    \n",
    "    \n",
    "    offset=0\n",
    "    i=0\n",
    "    while(i<10):\n",
    "        if('conv' in model.layers[i+offset].name):\n",
    "            model.layers[i+offset].set_weights(vgg_weights[i])\n",
    "            i=i+1\n",
    "            #print('h')\n",
    "            \n",
    "        else:\n",
    "            offset=offset+1\n",
    "\n",
    "    return (model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_loss(y_true, y_pred):\n",
    "    # Euclidean distance as a measure of loss (Loss function) \n",
    "    return K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network model : VGG + Conv\n",
    "def CrowdNet():  \n",
    "            #Variable Input Size\n",
    "            rows = None\n",
    "            cols = None\n",
    "            \n",
    "            #Batch Normalisation option\n",
    "            \n",
    "            batch_norm = 0\n",
    "            kernel = (3, 3)\n",
    "            init = RandomNormal(stddev=0.01)\n",
    "            model = Sequential() \n",
    "            \n",
    "            #custom VGG:\n",
    "            \n",
    "            if(batch_norm):\n",
    "                model.add(Conv2D(64, kernel_size = kernel, input_shape = (rows,cols,3),activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Conv2D(64, kernel_size = kernel,activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(MaxPooling2D(strides=2))\n",
    "                model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(MaxPooling2D(strides=2))\n",
    "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(MaxPooling2D(strides=2))            \n",
    "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                \n",
    "            else:\n",
    "                model.add(Conv2D(64, kernel_size = kernel,activation = 'relu', padding='same',input_shape = (rows, cols, 3), kernel_initializer = init))\n",
    "                model.add(Conv2D(64, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(MaxPooling2D(strides=2))\n",
    "                model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(MaxPooling2D(strides=2))\n",
    "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(MaxPooling2D(strides=2))            \n",
    "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                \n",
    "                \n",
    "\n",
    "                \n",
    "            #Conv2D\n",
    "            model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(256, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(128, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(64, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(1, (1, 1), activation='relu', dilation_rate = 1, kernel_initializer = init, padding = 'same'))\n",
    "        \n",
    "            sgd = SGD(lr = 1e-7, decay = (5*1e-4), momentum = 0.95)\n",
    "            model.compile(optimizer=sgd, loss=euclidean_distance_loss, metrics=['mse'])\n",
    "            \n",
    "            model = init_weights_vgg(model)\n",
    "            \n",
    "            return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = CrowdNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, None, None, 256)   1179904   \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, None, None, 128)   295040    \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, None, None, 64)    73792     \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, None, None, 1)     65        \n",
      "=================================================================\n",
      "Total params: 16,263,489\n",
      "Trainable params: 16,263,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = image_generator(img_paths,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr = 1e-7, decay = (5*1e-4), momentum = 0.95)\n",
    "model.compile(optimizer=sgd, loss=euclidean_distance_loss, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/500 [..............................] - ETA: 37s - loss: 0.3154 - mean_squared_error: 0.3905"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 197s 394ms/step - loss: 0.0769 - mean_squared_error: 0.0521\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_gen,epochs=1,steps_per_epoch= 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-6f2b69d28794>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#acc = history.history['acc']\n",
    "#val_acc = history.history['val_acc']\n",
    "\n",
    "loss = history.history['loss']\n",
    "#val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "#plt.subplot(2, 1, 1)\n",
    "#plt.plot(acc, label='Training Accuracy')\n",
    "#plt.plot(val_acc, label='Validation Accuracy')\n",
    "#plt.legend(loc='lower right')\n",
    "#plt.ylabel('Accuracy')\n",
    "#plt.ylim([min(plt.ylim()),1])\n",
    "#plt.title('Training and Validation Accuracy')\n",
    "\n",
    "#plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "#plt.plot(val_loss, label='Validation Loss')\n",
    "#plt.legend(loc='upper right')\n",
    "#plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,max(plt.ylim())])\n",
    "#plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_mod(model,\"/data/crowd_counting/weights/model_A_weights.h5\",\"/data/crowd_counting/models/ModelA.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
